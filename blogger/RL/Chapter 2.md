# 贝尔曼方程（Bellman）

上一章我们讲解了一些基础的定义，这一章我们讲解一下强化学习的核心贝尔曼方程，因为我们之前介绍过了两个概念，一个是状态价值函数，一个是动作价值函数，贝尔曼期望方程其实是我们之前的值函数的另一种表达形式，我们首先从状态价值函数开始：

## 状态价值函数的贝尔曼期望方程
首先我们将状态价值函数写成：
$$ Vπ(s)=Eπ​[Gt​∣St​=s]=Eπ​[k=0∑∞​γkRt+k+1​∣St​=s] $$
我们把第一个奖励项单独拆出来：
$$ ∑∞​γkRt+k+1​=Rt+1​+k=1∑∞​γkRt+k+1 $$
以及回报递推的公式：
$$ Gt​=Rt+1​+γGt+1 $$
所以我们的状态价值函数可以写成：
$$ Vπ(s)=Eπ​[Rt+1​+γGt+1​∣St​=s] \\
Vπ(s)=Eπ​[Rt+1​∣St​=s]+γEπ​[Gt+1​∣St​=s] $$
根据递推关系可以得到：
$$ Eπ​[Gt+1​∣St​=s]=Eπ​[Eπ​[Gt+1​∣St+1​]
​St​=s]=Eπ​[Vπ(St+1​)∣St​=s] $$
这里直观上的意义就是：从 $ t+1$ 开始的未来的回报，只取决于下一状态 $St+1$，其期望就是  $Vπ(St+1​)$

得到关键中间步：
$$ Vπ(s)=Eπ​[Rt+1​∣St​=s]+γEπ​[Vπ(St+1​)∣St​=s]​ $$
我们继续将期望写成