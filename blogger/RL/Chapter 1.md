# RL的起源

## MDP 的定义

我们从强化学习的基础定义开始，所有的强化学习起始于一个马尔可夫决策过程(markov decision process, MDP)，我们用公式表示就是：
$$M=(S,A,P,R,γ)$$
这里把我们称之为智能体agent的东西和环境进行交互的问题，抽象成为这样的一个公式，其中：

$S$：状态集合（state space）

$A$：动作集合（action space）

$P$：状态转移概率（transition kernel）

$R$：奖励机制（reward）

$γ∈[0,1)$：折扣因子（discount factor）

## State 和 Action

刚才的公式中的S和A我们暂时不用详细的讲述，用通俗的话来理解就是现在我出门去超市买菜，我的state可以是我走到家门口，还是在买菜的路上，还是已经在超市，任何在环境中的状态我们都可以称之为state，action就是我当前在某个state的时候我能够进行什么动作，我可以往前走，往后走，拿起一个物体等等，任何在当前state可以进行的操作我们都可以称之为action

大部分情况下，我们在算法中处理的问题都是离散的，LLM能够有token词表，下棋的下一步只能在有限的格子内，但是现实世界往往是连续的动作，自动驾驶，机器人的行动

一个很好的观点是所有的action其实都是一个PDF，我们对于不知道的动作空间，我们在采样中获得动作的PDF，连续动作就可以获得完整的PDF，而离散的动作可以算是特殊的PDF，或者使用PMF为了更符合数学上的定义

## 转移概率

MDP的公式中还有一个很重要的概念就是概率转移，概率转移其实表示的就是在当前state和action的情况下发生下一个state的概率，这个表达了markov性：下一步只和“当前状态 $s$ + 当前动作 $a$” 有关，不需要更久远的历史

$$p(s'∣s,a)=P(S_{t+1}​=s'∣S_t​=s,A_t​=a)$$

同时有：
$$∀s,a \space \sum_{s′∈S} ​p(s'∣s,a)=1$$

对于有限维的离散形式：若$|S|=n,|A|=m$，对于每个动作 $a$，都有一个转移矩阵：
$$P_a​∈R^{n×n},(P_a​)_{ij}​=p(s_j​∣s_i​,a)$$


## 奖励

奖励是对某个状态和动作的期望奖励，我们通产写成：
$$r(s,a)=E[R_{t+1}​∣S_t​=s,A_t​=a]$$
或者写成含有转移之后的state $s'$：
$$r(s,a,s')=E[R_{t+1​}∣S_t​=s,A_t​=a,S_{t+1}​=s']$$
可以通过公式互推：
$$r(s,a)=∑_{s'}​p(s'∣s,a)r(s,a,s')$$

## 回报(Return)

这里的return不是对某一个state的奖励，而是指的是累计得reward，定义是：从$t$时刻开始，未来所有奖励按照折扣因子$​γ$加权求和：

$$G_t​=∑_{k=0}^∞​γ^kR_{t+k+1}​$$

这个折扣因子
$$G_t​=R_{t+1}​+γG_{t+1}​​$$    