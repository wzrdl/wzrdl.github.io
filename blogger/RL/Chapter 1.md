# RL的起源

## MDP 的定义

我们从强化学习的基础定义开始，所有的强化学习起始于一个马尔可夫决策过程(markov decision process, MDP)，我们用公式表示就是：
$$M=(S,A,P,R,γ)$$
这里把我们称之为智能体agent的东西和环境进行交互的问题，抽象成为这样的一个公式，其中：

$S$：状态集合（state space）

$A$：动作集合（action space）

$P$：状态转移概率（transition kernel）

$R$：奖励机制（reward）

$γ∈[0,1)$：折扣因子（discount factor）

## State 和 Action

刚才的公式中的S和A我们暂时不用详细的讲述，用通俗的话来理解就是现在我出门去超市买菜，我的state可以是我走到家门口，还是在买菜的路上，还是已经在超市，任何在环境中的状态我们都可以称之为state，action就是我当前在某个state的时候我能够进行什么动作，我可以往前走，往后走，拿起一个物体等等，任何在当前state可以进行的操作我们都可以称之为action

大部分情况下，我们在算法中处理的问题都是离散的，LLM能够有token词表，下棋的下一步只能在有限的格子内，但是现实世界往往是连续的动作，自动驾驶，机器人的行动

一个很好的观点是所有的action其实都是一个PDF，我们对于不知道的动作空间，我们在采样中获得动作的PDF，连续动作就可以获得完整的PDF，而离散的动作可以算是特殊的PDF，或者使用PMF为了更符合数学上的定义

## 转移概率

MDP的公式中还有一个很重要的概念就是概率转移，概率转移其实表示的就是在当前state和action的情况下发生下一个state的概率，这个表达了markov性：下一步只和“当前状态 $s$ + 当前动作 $a$” 有关，不需要更久远的历史

$$p(s'∣s,a)=P(S_{t+1}​=s'∣S_t​=s,A_t​=a)$$

同时有：
$$∀s,a \space \sum_{s′∈S} ​p(s'∣s,a)=1$$

对于有限维的离散形式：若$|S|=n,|A|=m$，对于每个动作 $a$，都有一个转移矩阵：
$$P_a​∈R^{n×n},(P_a​)_{ij}​=p(s_j​∣s_i​,a)$$


## 奖励

奖励是对某个状态和动作的期望奖励，我们通产写成：
$$r(s,a)=E[R_{t+1}​∣S_t​=s,A_t​=a]$$
或者写成含有转移之后的state $s'$：
$$r(s,a,s')=E[R_{t+1​}∣S_t​=s,A_t​=a,S_{t+1}​=s']$$
可以通过公式互推：
$$r(s,a)=∑_{s'}​p(s'∣s,a)r(s,a,s')$$

## 回报(Return)

这里的return不是对某一个state的奖励，而是指的是累计得reward，定义是：从$t$时刻开始，未来所有奖励按照折扣因子$​γ$加权求和：

$$G_t​=∑_{k=0}^∞​γ^kR_{t+k+1}​$$

这个折扣因子，对后续的Bellman方程的推导有关键的作用，是所有价值函数递推的关系的源头
$$G_t​=R_{t+1}​+γG_{t+1}​​$$

# 价值函数

价值函数有两个，一个是状态价值函数，另一个是动作价值函数，这两个都是可以互相推导的，并且在给定了策略之后，整个轨迹的随机性来自于两个部分：
* 动作的随机性$At​∼π(⋅∣St​)$
* 环境的随机性$St+1​∼p(⋅∣St​,At​)$

## 状态价值函数 Value Function:

$$Vπ(s)=Eπ​[Gt​∣St​=s]=Eπ​[k=0∑∞​γkRt+k+1​∣St​=s]$$

直觉上来看，这个公式的意思就是，如果我现在正处在状态$s$，并且以后都按照策略$
\pi$行动，那么我“长期能拿到多少分”的期望是多少

从向量上理解就是：
$$Vπ∈Rn,Vπ=
​Vπ(s1​)⋮Vπ(sn​)​$$
这个对后续的Bellman方程的推导有用​

## 动作价值函数 Action Value
$$ Qπ(s,a)=Eπ​[Gt​∣St​=s,At​=a] $$

直觉上来看，这个公式的意思是，我现在正处在状态$s$，如果第一步我强制做动作$a$，后面再按照策略$\pi$走，长期的期望回报是多少

为什么我们再有了$Vπ(s)$之后，还需要这个$Qπ(s,a)$，因为在指定决策的时候，我们需要比较动作的好坏：
* $Vπ(s)$ 可以理解为平均表现
* $Qπ(s,a)$ 可以理解为某一个具体动作的表现
  
## V和Q之间的链路
$$ Vπ(s)=EA∼π(⋅∣s)​[Qπ(s,A)]=a∈A∑​π(a∣s)Qπ(s,a)$$
这个推导非常的直接，我们直接从定义展开：
* $Vπ$的定义上出发：
  $$ Vπ(s)=Eπ​[Gt​∣St​=s] $$
* 在条件 $ St​=s $ 的条件下，对第一步动作$A_t$ 做全概率展开
  $$ Eπ​[Gt​∣St​=s]=a∑​Pr(At​=a∣St​=s)Eπ​[Gt​∣St​=s,At​=a] $$
* 我们可以注意到
  $$ Pr(At​=a∣St​=s)=π(a∣s) $$
  $$ Eπ​[Gt​∣St​=s,At​=a]=Qπ(s,a) $$
得到
$$ Vπ(s)=a∑​π(a∣s)Qπ(s,a) $$

直观上来看这两者之间的关系就是：

状态价值 = 按照该策略在该状态下对“动作价值”的加权平均
